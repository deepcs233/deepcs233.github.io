<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 18px;
      zoom: 1
    }

    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 18px
    }
    /* small font */
    ps {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 18px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 18px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 28px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 20px;
      font-weight: 700;
    }

    detail {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      text-align: center;
      white-space: nowrap;
      font-size: 14px;
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 35px;
    }

    .small{
        line-height: 0.5em;
        font-size: 16px
    }

    .small-font{
        font-size: 18px
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <style type="text/css">
    #sectiontohide {
      padding: 20px;
      background: #f0f0f0;
      width: 400px;
    }
  </style>
  <script type="text/javascript">
    function toggle_div_fun(id) {

      var divelement = document.getElementById(id);

      if (divelement.style.display == 'none')
        divelement.style.display = 'block';
      else
        divelement.style.display = 'none';
    }
  </script>
  <link rel="icon" type="image/png" href="pictures/CUHK.png">
  <title>Hao Shao</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="1050" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="70%" valign="middle">
              <p align="center">
                <name>Hao Shao &nbsp; &nbsp; 邵昊</name>
              </p>
              <p>
                I am an first-year PhD student in <a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Laboratory</a> in the 
                Chinese University of Hong Kong. I'm supervised by  <a href="http://www.ee.cuhk.edu.hk/~hsli/">Prof. Hongsheng Li</a> and <a href="https://www.ee.cuhk.edu.hk/~xgwang/">Prof.Xiaogang Wang</a>. 
              </p>
              <p>
                  Before that, I received my Master's degree from Tsinghua University in 2022,
                  and  my Bachelor degree from the University of Electronic Science and Technology of China
                in 2019.
              </p>
              <p>
                My research interests lie in the area of Autonomous Driving and Computer Vision. 
                Specifically, I'm pariticularly interested in end-to-end autonomous driving, trajectory prediction and video understanding. 
              </p>
              <p align=center>
                <a href="mailto:shaohao97@gmail.com"> Email </a> /
                <!-- <a href="https://www.linkedin.com/in/xudong-xu-5bb7b3173/"> LinkedIn </a> /-->
                <a href="https://scholar.google.com.hk/citations?user=D_ZLR1oAAAAJ"> Google Scholar </a> /
                <a href="https://github.com/deepcs233"> Github </a>
              </p>
            </td>
            <td width=25%>
            <div  style="display: flex; flex-direction: column; align-items: center;">
              <img src="pictures/shaohao.png" width="200">
              <detail style="">Mount Gongga, Sichuan</detail>
              <div>
              </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Education</heading>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="10">
          <tr>
            <td width="25%" align="center">
              <img src="pictures/CUHK.png" alt="CUHK" width="95" height="75">
            </td>
            <td width="75%" valign="top">
              <p>
                <strong>Aug. 2023 -   </strong>, Department of Electronic Engineering,
                <i>
                  <b>the Chinese University of Hong Kong</b>
                </i></p>
              <p>PhD Student</br>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="10">
          <tr>
            <td width="25%" align="center">
              <img src="pictures/thu.png" alt="THU" width="90" height="90">
            </td>
            <td width="75%" valign="top">
              <p>
                <strong>Sept. 2019 - Jun. 2022 </strong>, School of Software Engineering,
                <i>
                  <b>Tsinghua University</b>
                </i></p>
              <p>Master</br>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="10">
          <tr>
            <td width="25%" align="center">
              <img src="pictures/uestc.png" alt="UESTC" width="90" height="90">
            </td>
            <td width="75%" valign="top">
              <p>
                <strong>Sept. 2015 - Jun. 2019 </strong>, School of Software Engineering,
                <i>
                  <b>University of Electronic Science and Technology of China</b>
                </i></p>
              <p>Bachelor   GPA: 3.98/4</br>
              </p>
            </td>
          </tr>
        </table>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </table>

	<table width="100%" align="center" border="0" cellspacing="10" cellpadding="10">
          <tr>
            <td width="30%" align="center">
              <img src="pictures/viscot.png" alt="viscot" width="300">
            </td>
            <td width="70%" valign="top">
              <p>
                <papertitle>Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning</papertitle>
                <br>
		<strong>Hao Shao</strong>, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, Hongsheng Li
                <br>
                <em>Neural Information Processing Systems (<strong>Neurips</strong>)</em>, <font color="#FF0000">(Spotlight)</font>, 2024
                <br>
                <a href="https://arxiv.org/abs/2403.16999">[paper]</a>
                <a href="https://github.com/deepcs233/Visual-CoT">[code]</a>
                <p>
                   We propose Visual CoT, including a new pipeline/dataset/benchmark that enhances the interpretability of MLLMs by incorporating visual Chain-of-Thought reasoning, optimizing for complex visual inputs.
                </p>
            </td>
          </tr>
        </table>

	<table width="100%" align="center" border="0" cellspacing="10" cellpadding="10">
          <tr>
            <td width="30%" align="center">
              <img src="pictures/lmdrive.png" alt="lmdrive" width="300">
            </td>
            <td width="70%" valign="top">
              <p>
                <papertitle>LMDrive: Closed-Loop End-to-End Driving with Large Language Models</papertitle>
                <br>
                <strong>Hao Shao</strong>, Yuxuan Hu, Letian Wang, Guanglu Song, Steven L. Waslander, Yu Liu, Hongsheng Li
                <br>
                <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
                <br>
                <a href="./projects/lmdrive.html">[website]</a>
                <a href="https://arxiv.org/abs/2312.07488">[paper]</a>
                <a href="https://github.com/opendilab/LMDrive">[code]</a>
                <p>
                   We propose a novel end-to-end, closed-loop, language-based autonomous driving framework, LMDrive, which interacts with the dynamic environment via multi-modal multi-view sensor data and natural language instructions.
                </p>
            </td>
          </tr>
        </table>


	<table width="100%" align="center" border="0" cellspacing="10" cellpadding="10">
          <tr>
            <td width="30%" align="center">
              <img src="pictures/smartrefine.png" alt="smartrefine" width="300">
            </td>
            <td width="70%" valign="top">
              <p>
                <papertitle>SmartRefine: A Scenario-Adaptive Refinement Framework for Efficient Motion Prediction</papertitle>
                <br>
		Yang Zhou*, <strong>Hao Shao*</strong>, Letian Wang, Steven L. Waslander, Hongsheng Li, Yu Liu
                <br>
                <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2403.11492">[paper]</a>
                <a href="https://github.com/opendilab/SmartRefine/">[code]</a>
                <p>
                   We introduce a novel scenario-adaptive refinement strategy to refine trajectory prediction with minimal additional computation.
                </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="10" cellpadding="10">
          <tr>
            <td width="30%" align="center">
              <img src="pictures/asprl2023.png" alt="asprl" width="300">
            </td>
            <td width="70%" valign="top">
              <p>
                <papertitle>Efficient Reinforcement Learning for Autonomous Driving with Parameterized Skills and Priors</papertitle>
                <br>
                Letian Wang, Jie Liu, <strong>Hao Shao</strong>, Wenshuo Wang, Ruobing Chen, Yu Liu, Steven L Waslander
                <br>
                <em>Robotics: Science and Systems (<strong>RSS</strong>)</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2305.04412">[paper]</a>
                <a href="https://github.com/Letian-Wang/asaprl">[code]</a>
                <p>
                   We present an efficient reinforcement learning (ASAP-RL) that simultaneously leverages parameterized motion skills and expert priors for autonomous vehicles to navigate in complex dense traffic.
                </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="10" cellpadding="10">
          <tr>
            <td width="30%" align="center">
              <img src="pictures/reasonnet2023.png" alt="reasonnet" width="300">
            </td>
            <td width="70%" valign="top">
              <p>
                <papertitle>ReasonNet: End-to-End Driving with Temporal and Global Reasoning</papertitle>
                <br>
                <strong>Hao Shao</strong>, Letian Wang, Ruobing Chen, Steven L Waslander, Hongsheng Li, Yu Liu
                <br>
                <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shao_ReasonNet_End-to-End_Driving_With_Temporal_and_Global_Reasoning_CVPR_2023_paper.html">[paper]</a>
                <a href="https://github.com/opendilab/DOS">[code]</a>
                <br>
                <p>
                    We present ReasonNet, a novel end-to-end driving framework that extensively exploits both temporal and global information of the driving scene.
                </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="10" cellpadding="10">
          <tr>
            <td width="30%" align="center">
              <img src="pictures/interfuser2022.png" alt="interfuser" width="300">
            </td>
            <td width="70%" valign="top">
              <p>
                <papertitle>Safety-enhanced autonomous driving using interpretable sensor fusion transformer</papertitle>
                <br>
                <strong>Hao Shao</strong>, Letian Wang, Ruobing Chen, Hongsheng Li, Yu Liu
                <br>
                <em>Conference on Robot Learning (<strong>CoRL</strong>)</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2207.14024">[paper]</a>
                <a href="https://github.com/opendilab/InterFuser">[code]</a>
                <br>
                <p>
                    We propose a safety-enhanced autonomous driving framework to fully process and fuse information from multi-modal multi-view sensors for achieving comprehensive scene understanding and adversarial event detection.
                </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="10" cellpadding="10">
          <tr>
            <td width="30%" align="center">
              <img src="pictures/antialising2021.png" alt="antialiasing" width="300">
            </td>
            <td width="70%" valign="top">
              <p>
                <papertitle>Blending anti-aliasing into vision transformer</papertitle>
                <br>
                Shengju Qian, <strong>Hao Shao</strong>, Yi Zhu, Mu Li, Jiaya Jia
                <br>
                <em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2021
                <br>
                <a href="https://proceedings.neurips.cc/paper/2021/hash/2b3bf3eee2475e03885a110e9acaab61-Abstract.html">[paper]</a>
                <br>
                <p>
                    We propose a plug-and-play Aliasing-Reduction Module (ARM) to alleviate the problem of aliasing in vision transformer.
                </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="10" cellpadding="10">
          <tr>
            <td width="30%" align="center">
              <img src="pictures/tin2020.png" alt="tin" width="300">
            </td>
            <td width="70%" valign="top">
              <p>
                <papertitle>Temporal interlacing network</papertitle>
                <br>
                <strong>Hao Shao</strong>, Shengju Qian, Yu Liu
                <br>
                <em>AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>, 2020
                <br>
                <a href="https://arxiv.org/abs/2001.06499">[paper]</a>
                <a href="https://github.com/deepcs233/TIN">[code]</a>
                <br>
                <p>
                    We present a simple yet powerful operator – temporal interlacing network (TIN). TIN fuses the two kinds of information by interlacing spatial representations from the past to the future, and vice versa.
                </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Industry Experience</heading>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="10">
          <tr>
            <td width="25%" align="center">
              <img src="pictures/sensetime.png" alt="Sensetime" width="180" height="95">
            </td>
            <td width="75%" valign="top">
              <p>
                <strong>Apr 2019 - Now</strong>,
                <i>
                  <b>XLab</b>
                </i></p>
                Researcher(intern).  <i>Beijing, China</i>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="10">
          <tr>
            <td width="25%" align="center">
              <img src="pictures/tencent.png" alt="Tencent" width="220" height="95">
            </td>
            <td width="75%" valign="top">
              <p>
                <strong>Sep 2018 - Apr 2019</strong>,
                <i>
                  <b>Computer Vision</b>
                </i></p>
                Research intern.  <i>Shenzhen, China</i>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="10">
          <tr>
            <td width="25%" align="center">
              <img src="pictures/bytedance.png" alt="ByteDance" width="200" height="100">
            </td>
            <td width="75%" valign="top">
              <p>
                <strong>Jul 2017 - May 2018</strong>,
                <i>
                  <b>Recommend System</b>
                </i></p>
                Research intern.  <i>Beijing, China</i>
              </p>
            </td>
          </tr>
        </table>

        <!--
        <table width="100%" align="center" border="0" cellspacing="10" cellpadding="20">
          <tr>
            <td>
              <heading>Academic Activities</heading>
            </td>
          </tr>
        </table>
        <ul>
          <li>
            <p>I serve as a reviewer for BMVC, CVPR, ICCV, NeurIPS, ICML, AAAI, etc.
          </li>
        </ul>
        -->
        <!-- Honors and Awards ======================================== -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Honors and Awards</heading>
            </td>
          </tr>
        </table>
        <ul>
          <li>
            <p class="small">Postgraduate Scholarship, the Chinese University of Hong Kong, 2023 ~ now</p>
          </li>
          <li>
              <p class="small">The First Prize, <a class="small" href="https://leaderboard.carla.org/leaderboard/"> CARLA Autonomous Driving Challenge (Sensor track)</a>, 2022</p>
          </li>
          <li>
            <p class="small">The First Prize, CVPR20 ActivityNet Challenge (<a class="small" href="http://activity-net.org/challenges/2020/tasks/guest_kinetics.html/">Kinetics700</a> track and  <a class="small" href="https://www.youtube.com/watch?v=zJPEmG3LCH4&amp;list=PLw6H4u-XW8siSxqdRVcD5aBn3OTuA7M7x/">AVA track</a>), 2020</p>
          </li>
          <li>
            <p class="small">The First Prize, <a  class="small"href="http://moments.csail.mit.edu/results2019.html/">ICCV19 Multi-Moments in Time (MIT) Challenge</a>, 2019</p>
          </li>
          <li>
            <p class="small">Outstanding Graduate of UESTC, 2019</p>
          </li>
          <li>
            <p class="small">National Scholarship, University of Electronic Science and Technology of China, 2017</p>
          </li>
        </ul>
        <br />


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Projects</heading>
            </td>
          </tr>
        </table>
        <ul>
          <li>
              <p class="small"><a class="small" href="https://github.com/Sense-X/X-Temporal">X-Temporal</a> <img class="small" alt="stars" src="https://img.shields.io/github/stars/Sense-X/X-Temporal?style=social" /> , Easily implement SOTA video understanding methods with PyTorch on multiple machines and GPUs</p>
          </li>
          <li>
              <p class="small"><a  class="small"href="https://github.com/opendilab/awesome-end-to-end-autonomous-driving">Awesome End-to-End Autonomous Driving</a> <img class="small" alt="stars" src="https://img.shields.io/github/stars/opendilab/awesome-end-to-end-autonomous-driving?style=social" /> , Paper list about end-to-end autonomous driving</p>
          </li>
          <li>
              <p class="small"><a class="small" href="https://github.com/opendilab/DI-drive">DI-drive</a> <img class="small" alt="stars" src="https://img.shields.io/github/stars/opendilab/DI-drive?style=social" /> , Decision Intelligence Platform for Autonomous Driving simulation</p>
          </li>
          <li>
              <p class="small"><a class="small" href="https://github.com/deepcs233/jieba_fast">Fast Jieba</a> <img  class="small"alt="stars" src="https://img.shields.io/github/stars/deepcs233/jieba_fast?style=social" /> , Fast Chinese word segmentation library, rewriting Jieba core functions (<i>accumulated 138K downloads</i>)</p>
          </li>
        </ul>
        <br />



        <!-- Footer ================================================== -->
        <hr>

        <footer class="footer">
          <div class="container">
            <p>
              &nbsp;&nbsp;&nbsp;Updated Jul. 2024</p>
            </p>
            <p align=right>
              <a href="https://ai.stanford.edu/~kaidicao/"> Page Template </a>
              <a class="pull-right" href="#">
                  <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=500&t=tt&d=VkYT7USR1EDQkDTDlP7q_8FwHSe_Y6-QtMU3WeV4Wgc&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff"></script>
              </a>
            </p>
          </div>
        </footer>

		<!-- Google tag (gtag.js) -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-SCMVBJM8DX"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'G-SCMVBJM8DX');
		</script>

      </td>
    </tr>
  </table>
</body>

</html>
